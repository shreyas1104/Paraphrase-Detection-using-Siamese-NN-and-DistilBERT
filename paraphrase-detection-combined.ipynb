{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15301,"status":"ok","timestamp":1691634645318,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"azEg_9C_Djtu","outputId":"bdc299dd-3112-453c-f373-62e939acc9bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Download required NLTK resources and import libraries\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.optimizers import Adam\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","import matplotlib.pyplot as plt\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pgUg_qZk0lU"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ssvOGFrYfzGJ"},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58734,"status":"ok","timestamp":1691634716789,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"TH3VK90_f1-w","outputId":"f9df3b26-2912-44f5-92ec-adda5c2d09b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 0.5119\n"]}],"source":["# Load the dataset\n","\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the text data\n","def preprocess_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    preprocessed_tokens = [token.lower() for token in tokens if token.isalpha()]\n","    return ' '.join(preprocessed_tokens)\n","\n","train_df['preprocessed_sentence1'] = train_df['sentence1'].apply(preprocess_text)\n","train_df['preprocessed_sentence2'] = train_df['sentence2'].apply(preprocess_text)\n","test_df['preprocessed_sentence1'] = test_df['sentence1'].apply(preprocess_text)\n","test_df['preprocessed_sentence2'] = test_df['sentence2'].apply(preprocess_text)\n","\n","# Create feature vectors using TF-IDF\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train_df['preprocessed_sentence1'] + ' ' + train_df['preprocessed_sentence2'])\n","X_test = vectorizer.transform(test_df['preprocessed_sentence1'] + ' ' + test_df['preprocessed_sentence2'])\n","\n","# Scale the dense feature matrices (use with_mean=False to avoid centering sparse matrices)\n","scaler = StandardScaler(with_mean=False)\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Prepare the target variable\n","y_train = train_df['label']\n","y_test = test_df['label']\n","\n","# Train a logistic regression model\n","lr_model = LogisticRegression(max_iter=5000)  # Increase the number of iterations\n","lr_model.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred = lr_model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","lr_accuracy = round(accuracy_score(y_test, y_pred),4)\n","print(\"Logistic Regression Accuracy:\", lr_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"doDeie7ff39F"},"source":["### Siamese NN"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":675428,"status":"ok","timestamp":1691635392209,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"eTcX2TWGf756","outputId":"259b9cc1-9f01-4716-ea3f-eb2c76796112"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training started...\n","Epoch 1/50\n","897/897 [==============================] - 60s 54ms/step - loss: 0.6819 - accuracy: 0.5576\n","Epoch 2/50\n","897/897 [==============================] - 12s 14ms/step - loss: 0.6732 - accuracy: 0.5580\n","Epoch 3/50\n","897/897 [==============================] - 11s 13ms/step - loss: 0.6634 - accuracy: 0.5581\n","Epoch 4/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.6512 - accuracy: 0.5555\n","Epoch 5/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.6262 - accuracy: 0.5828\n","Epoch 6/50\n","897/897 [==============================] - 12s 13ms/step - loss: 0.5991 - accuracy: 0.6188\n","Epoch 7/50\n","897/897 [==============================] - 10s 12ms/step - loss: 0.5683 - accuracy: 0.6567\n","Epoch 8/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.5371 - accuracy: 0.6931\n","Epoch 9/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.4900 - accuracy: 0.7425\n","Epoch 10/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.4132 - accuracy: 0.8134\n","Epoch 11/50\n","897/897 [==============================] - 9s 9ms/step - loss: 0.3631 - accuracy: 0.8501\n","Epoch 12/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.3033 - accuracy: 0.8825\n","Epoch 13/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.2233 - accuracy: 0.9211\n","Epoch 14/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.1798 - accuracy: 0.9381\n","Epoch 15/50\n","897/897 [==============================] - 11s 13ms/step - loss: 0.1564 - accuracy: 0.9470\n","Epoch 16/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.1382 - accuracy: 0.9538\n","Epoch 17/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.1261 - accuracy: 0.9588\n","Epoch 18/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.1181 - accuracy: 0.9618\n","Epoch 19/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.1109 - accuracy: 0.9647\n","Epoch 20/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.1049 - accuracy: 0.9660\n","Epoch 21/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0995 - accuracy: 0.9682\n","Epoch 22/50\n","897/897 [==============================] - 10s 12ms/step - loss: 0.0938 - accuracy: 0.9700\n","Epoch 23/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0895 - accuracy: 0.9718\n","Epoch 24/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0851 - accuracy: 0.9726\n","Epoch 25/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0814 - accuracy: 0.9742\n","Epoch 26/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0774 - accuracy: 0.9759\n","Epoch 27/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0761 - accuracy: 0.9760\n","Epoch 28/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0737 - accuracy: 0.9770\n","Epoch 29/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0711 - accuracy: 0.9771\n","Epoch 30/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0690 - accuracy: 0.9781\n","Epoch 31/50\n","897/897 [==============================] - 15s 17ms/step - loss: 0.0678 - accuracy: 0.9783\n","Epoch 32/50\n","897/897 [==============================] - 13s 15ms/step - loss: 0.0657 - accuracy: 0.9795\n","Epoch 33/50\n","897/897 [==============================] - 11s 13ms/step - loss: 0.0642 - accuracy: 0.9803\n","Epoch 34/50\n","897/897 [==============================] - 8s 9ms/step - loss: 0.0640 - accuracy: 0.9798\n","Epoch 35/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0608 - accuracy: 0.9807\n","Epoch 36/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0596 - accuracy: 0.9813\n","Epoch 37/50\n","897/897 [==============================] - 11s 12ms/step - loss: 0.0588 - accuracy: 0.9810\n","Epoch 38/50\n","897/897 [==============================] - 19s 22ms/step - loss: 0.0570 - accuracy: 0.9822\n","Epoch 39/50\n","897/897 [==============================] - 16s 18ms/step - loss: 0.0560 - accuracy: 0.9817\n","Epoch 40/50\n","897/897 [==============================] - 13s 15ms/step - loss: 0.0552 - accuracy: 0.9820\n","Epoch 41/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0546 - accuracy: 0.9824\n","Epoch 42/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0542 - accuracy: 0.9829\n","Epoch 43/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0525 - accuracy: 0.9827\n","Epoch 44/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0512 - accuracy: 0.9839\n","Epoch 45/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0514 - accuracy: 0.9834\n","Epoch 46/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0501 - accuracy: 0.9837\n","Epoch 47/50\n","897/897 [==============================] - 9s 10ms/step - loss: 0.0495 - accuracy: 0.9839\n","Epoch 48/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0499 - accuracy: 0.9841\n","Epoch 49/50\n","897/897 [==============================] - 11s 13ms/step - loss: 0.0494 - accuracy: 0.9842\n","Epoch 50/50\n","897/897 [==============================] - 10s 11ms/step - loss: 0.0484 - accuracy: 0.9846\n","Testing started...\n","250/250 [==============================] - 2s 4ms/step\n","Siamese NN Accuracy: 0.6105\n"]}],"source":["# Configure GPU memory growth\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the text data\n","def preprocess_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [token.lower() for token in tokens if token.isalpha()]\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token not in stop_words]\n","    return ' '.join(tokens)\n","\n","# Apply text preprocessing to the dataset\n","train_df['preprocessed_sentence1'] = train_df['sentence1'].apply(preprocess_text)\n","train_df['preprocessed_sentence2'] = train_df['sentence2'].apply(preprocess_text)\n","test_df['preprocessed_sentence1'] = test_df['sentence1'].apply(preprocess_text)\n","test_df['preprocessed_sentence2'] = test_df['sentence2'].apply(preprocess_text)\n","\n","# Create vocabulary and tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df['preprocessed_sentence1'] + train_df['preprocessed_sentence2'])\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Convert sentences to sequences\n","train_seq1 = tokenizer.texts_to_sequences(train_df['preprocessed_sentence1'])\n","train_seq2 = tokenizer.texts_to_sequences(train_df['preprocessed_sentence2'])\n","test_seq1 = tokenizer.texts_to_sequences(test_df['preprocessed_sentence1'])\n","test_seq2 = tokenizer.texts_to_sequences(test_df['preprocessed_sentence2'])\n","\n","# Pad sequences\n","max_seq_length = 50\n","train_seq1 = pad_sequences(train_seq1, maxlen=max_seq_length, padding='post')\n","train_seq2 = pad_sequences(train_seq2, maxlen=max_seq_length, padding='post')\n","test_seq1 = pad_sequences(test_seq1, maxlen=max_seq_length, padding='post')\n","test_seq2 = pad_sequences(test_seq2, maxlen=max_seq_length, padding='post')\n","\n","# Prepare the target variable\n","y_train = train_df['label']\n","y_test = test_df['label']\n","\n","# Siamese neural network model\n","embedding_dim = 100\n","lstm_units = 64\n","\n","input1 = Input(shape=(max_seq_length,))\n","input2 = Input(shape=(max_seq_length,))\n","\n","# Embedding layer to convert words to dense vectors\n","embedding_layer = Embedding(vocab_size, embedding_dim)\n","\n","# LSTM layer to process sequences\n","lstm_layer = LSTM(lstm_units)\n","\n","# Process inputs through embedding and LSTM layers\n","encoded1 = lstm_layer(embedding_layer(input1))\n","encoded2 = lstm_layer(embedding_layer(input2))\n","\n","# Calculate absolute difference between encoded vectors\n","merged = Lambda(lambda x: abs(x[0] - x[1]))([encoded1, encoded2])\n","\n","# Predict the probability of paraphrase\n","preds = Dense(1, activation='sigmoid')(merged)\n","\n","# Create the Siamese model\n","siamese_model = Model(inputs=[input1, input2], outputs=preds)\n","siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training\n","print(\"Training started...\")\n","siamese_model.fit([train_seq1, train_seq2], y_train, epochs=50, batch_size=64, verbose=1)\n","\n","# Testing\n","print(\"Testing started...\")\n","y_pred = siamese_model.predict([test_seq1, test_seq2])\n","\n","# Convert predicted probabilities to binary predictions\n","y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n","\n","# Evaluate the model\n","snn_accuracy = round(accuracy_score(y_test, y_pred),4)\n","print(\"Siamese NN Accuracy:\", snn_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"-pN7EbrxhZMY"},"source":["### DistilBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqtYZoYLhhBG","outputId":"8beb8fe4-da40-47c9-88f7-1ee78cf939fe"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1:   0%|          | 15/3588 [00:02<11:10,  5.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  29%|██▊       | 1027/3588 [03:28<08:26,  5.06it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  50%|████▉     | 1787/3588 [05:55<06:01,  4.98it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  52%|█████▏    | 1868/3588 [06:11<05:22,  5.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1: 100%|█████████▉| 3573/3588 [11:39<00:02,  5.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1: 100%|██████████| 3588/3588 [11:42<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 Loss: 0.5498794106291057\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:  16%|█▋        | 591/3588 [01:53<09:20,  5.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  20%|█▉        | 701/3588 [02:14<09:01,  5.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  23%|██▎       | 829/3588 [02:39<08:39,  5.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  70%|██████▉   | 2498/3588 [08:00<03:43,  4.87it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  71%|███████   | 2554/3588 [08:11<03:17,  5.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  88%|████████▊ | 3172/3588 [10:10<01:18,  5.28it/s]"]}],"source":["# Load the training and test data\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the data\n","X_train = train_df[['sentence1', 'sentence2']]\n","y_train = train_df['label']\n","X_test = test_df[['sentence1', 'sentence2']]\n","y_test = test_df['label']\n","\n","# Define a custom dataset for PyTorch\n","class ParaphraseDataset(Dataset):\n","    def __init__(self, tokenizer, sentences1, sentences2, labels):\n","        self.sentences1 = sentences1\n","        self.sentences2 = sentences2\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        sentence1 = self.sentences1.iloc[idx]\n","        sentence2 = self.sentences2.iloc[idx]\n","        label = self.labels.iloc[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            sentence1,\n","            sentence2,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            max_length=128,\n","            truncation=True\n","        )\n","\n","        input_ids = encoding[\"input_ids\"].squeeze()\n","        attention_mask = encoding[\"attention_mask\"].squeeze()\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": label\n","        }\n","\n","# Initialize the tokenizer and model\n","distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","# Move the model to the GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","distilbert_model.to(device)\n","\n","# Create DataLoader objects for the training and test datasets with increased batch size\n","train_dataset = ParaphraseDataset(distilbert_tokenizer, X_train[\"sentence1\"], X_train[\"sentence2\"], y_train)\n","test_dataset = ParaphraseDataset(distilbert_tokenizer, X_test[\"sentence1\"], X_test[\"sentence2\"], y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16)\n","\n","# Fine-tuning and Training the model\n","distilbert_model.train()\n","optimizer = AdamW(distilbert_model.parameters(), lr=2e-5)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1} Loss: {running_loss / len(train_dataloader)}\")\n","\n","\n","# Save the fine-tuned model\n","model_save_path = \"distilbert_model.pth\"\n","# from google.colab import files\n","# files.download('distilbert_model.pth')\n","torch.save(distilbert_model.state_dict(), model_save_path)\n","\n","# Load the fine-tuned model\n","distilbert_model.load_state_dict(torch.load(model_save_path))\n","distilbert_model.eval()\n","\n","# Testing the model\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","# Convert predictions from numerical to binary labels (0 or 1)\n","y_pred = [1 if pred == 1 else 0 for pred in y_pred]\n","\n","# Compute performance metrics\n","db_accuracy = round(accuracy_score(y_true, y_pred),4)\n","print(\"DistilBERT Accuracy:\", db_accuracy)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
